---
title: "R Notebook"
output: html_notebook
---


```{r}
rm(list=ls())

# Settings
settingImputation = T

set.seed(03031988)
library(data.table)

if (!exists('train')) { train <- data.table(read.csv('train.csv', na.strings="-1")) }
if (!exists('test')) { test <- data.table(read.csv('test.csv', na.strings="-1")) }

```

## Data transformation
Info from the Kaggle page:
* The **_bin** suffix means that it is a dummy variable. 
* The **_cat** suffix means that it is a categorical value.
* **-1** means that the value is missing. This has already been taken care of when loading the data.

```{r}
# Transform categorical values - train
cols <- grep("\\_cat",names(train),value=T)
train[, (cols) := lapply(.SD, as.factor), .SDcols=cols]
# train[cols] <- lapply(train[cols],as.factor)


# Transform boolean values - train (also target)
cols <- grep("\\_bin|target",names(train),value=T)
train[, (cols) := lapply(.SD, as.logical), .SDcols=cols]

# Transform categorical values - test
cols <- grep("\\_cat",names(train),value=T)
test[, (cols) := lapply(.SD, as.factor), .SDcols=cols]

# Transform boolean values - test
cols <- grep("\\_bin",names(train),value=T)
test[, (cols) := lapply(.SD, as.logical), .SDcols=cols]

rm(cols)

# Change target classification to factor for caret package
train$target <- as.factor(train$target)

# Remove id
if ("id" %in% names(train)) {
    train <- subset(train,select=c(-id))
}

# Make backup of train
save(train,file="train.rda")
```

## Exploratory analysis

The fact that the claim rate is very low can make the prediction very hard. The _specificity_ will probably be very high, but the _sensitivity_ won't be.

```{r}
# Dimensions of the data frame
dim(train)

# Names of the variables
names(train)

# Summary and overview of the data
str(train)

# What is the 'claim rate'?
nrow(train[target==TRUE]) / nrow(train)

```

How many complete rows are there?

There is also quite a lot of rows which contain NA's. Interesting is the fact that the 'claim rate' is higher for rows where there is no NA values.
```{r}
# Absolute
nrow(train[complete.cases(train),])

# Relative
nrow(train[complete.cases(train),]) / nrow(train)

# Is the amount of NAs spread evenly in relation to the 'claim rate'?

nrow(train[complete.cases(train) & target == TRUE,]) / nrow(train[complete.cases(train),])
nrow(train[!complete.cases(train) & target == TRUE,]) / nrow(train[!complete.cases(train),])

```


## Imputation
In this phase we impute the missing data using a KNN method. We also save the meta data - the information that variable Y was missing for row X - in a new variable. We do this for each variable.

```{r}
library(RANN)
library(caret)
if (settingImputation == T) {
    if (file.exists("trainImputed.rda")) {
        load(file="trainImputed.rda")
    } else {
        # This is a loop that makes sure that the meta data 'field was an NA' is stored in a 
        # separate new dummy for each of the existing variables.
        
        for (i in 2:as.integer(ncol(train))) {
            isNA <- data.frame(is.na(train[,i,with=F])) # I'm unsure why I need to use 'with' here, I though this was only needed in old data.table versions
            names(isNA) <- paste0("is_na_",names(train)[i])
            if (i == 2) { trainNA <- isNA }
            else { trainNA <- cbind(trainNA,isNA) }
        }
        
        # In a next phase we impute these with a KNN method.
        knnImputeModel <- caret::preProcess(train,method="knnImpute")
        trainTest <- predict(knnImputeModel,train,100)
        
        # Save the object to avoid recalculation
        # save(train,"trainImputed.rda")
    }
}
```

## Fitting all variables
For education purposes, we simple modeled a logit model on all the existing variables.

```{r}
# lmAll <- glm(target~.,data=train)
# summary(lmAll)
```

## Feature selection
### Filter method
#### Correlation matrix
Here is a correlation matrix. The following variables are highly correlated (arbitrary cutoff at 0.6):
* **ps_ind_14** and ps_ind_12_bin
* ps_reg_01 and ps_reg_03
* ps_car_13 and ps_car_12
* ps_ind_17_bin and ps_ind_16_bin
* **ps_ind_14** and ps_ind_11_bin

```{r}
# Remove categorical and target column
cols <- grep("\\_cat|target",names(train),value=T,invert=T)

# Generate correlation matrix with absolute value of correlation
corMatrixWide <- abs(cor(train[,..cols]))

# Convert correlation matrix to long format
corMatrixLong <- data.table(melt(corMatrixWide))

# Remove self-correlation rows
corMatrixLong <- corMatrixLong[Var1 != Var2]

# Order from highly-correlate to not correlatied
corMatrixLong <- corMatrixLong[order(-value),]

# Remove dupes (DANGER - what if different covariates have same correlation coefficient?)
corMatrixLong <- corMatrixLong[-seq(0,nrow(corMatrixLong),2),]

# Remove highly correlated values
abundantFeatures <- as.character(unique(corMatrixLong[value > 0.6,]$Var2))
trainF <- train[, (abundantFeatures) := NULL]

# Clean up
rm(cols)
rm(abundantFeatures)
```

#### Decision trees
To find the most important features, we can use decision trees. However, the algorithm does not appear to find any significant features as it is only producing a root. Which is very problematic because decision trees are generally a good way to classify imbalanced classes.

```{r}
library(rpart)
# trainTree <- rpart(target~.,data=trainF,method='class',control=rpart.control(minsplit=1, minbucket=1, cp=0.001))
# plot(trainTree)
```

#### Individual logit regression
In the following section, I run a bivariate logit regression for every variable to determine the importance of each variable individually. This also turns out not to be very informative.

```{r}
glmVars <- data.table(variable = character(),AIC=integer(),deviance=integer(),p=integer())
for (i in 2:ncol(trainF)) {
    v <- names(trainF)[i]
    model <- glm(formula(paste("target~",v)),family="binomial",data=trainF)
    modelSum <- summary(model)
    newrow <- data.table(variable=v,AIC=modelSum$aic,deviance=modelSum$deviance,p=modelSum$coefficients[8])
    glmVars <- rbind(glmVars,newrow)
}
glmVars <- glmVars[order(deviance),]

# Clean up
rm(model)
rm(modelSum)
rm(newrow)
rm(v)
```

### Wrapper method

We can use the bestglm or regsubsets package to find out the best models. This is slow as hell because it is a huge data set, even when sampled, as it has 2^57 models.

```{r}
library(regsubsets)
# lmReg <- regsubsets(target~.,data=train,nvmax=57,really.big=T)
```

What about forward and backward feature selection. Also very slow. Time to go to other solution.

```{r}
# Backward
# lmBwd <- regsubsets(target~.,data=train,method="backward")
```

## Random Forest with Undersampling

The following settings are used:
* na.action = na.omit (no imputation)
* 3 fold cross-valdiation with one repeat
* Undersampling (!)

Sensitivity and specificity around 62% (which is bad).

```{r}
table(train$target)

# ROSE package does not seem to work at all.
# mlr package can't handle logical columns.
# Not a big fan of caret package because it dumbs things down too much. But here goes.
library(caret)
rfModel <- caret::train(target ~ ., data=train, method = "rf", 
                        preProcess = c("scale","center"), na.action = na.omit,
                        trControl = trainControl(method = "repeatedcv",number = 3,
                                                 repeats = 1, verboseIter=T,
                                                 sampling="down")) # Undersampling

rfTrainPredict <- predict(rfModel,train)
confusionMatrix(rfTrainPredict,train[complete.cases(train),]$target)
varImp(rfModel,scale=T)
```