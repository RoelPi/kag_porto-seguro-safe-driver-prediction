---
title: "R Notebook"
output: html_notebook
---


```{r}
set.seed(03031988)
library(data.table)

# Comment this out when data does not need to be reloaded
rm(train)

if (!exists('train')) { train <- data.table(read.csv('train.csv', na.strings="-1")) }
if (!exists('test')) { test <- data.table(read.csv('test.csv', na.strings="-1")) }

```

## Data transformation
Info from the Kaggle page:
* The **_bin** suffix means that it is a dummy variable. 
* The **_cat** suffix means that it is a categorical value.
* **-1** means that the value is missing. This has already been taken care of when loading the data.

```{r}
# Transform categorical values - train
cols <- grep("\\_cat",names(train),value=T)
train[, (cols) := lapply(.SD, as.factor), .SDcols=cols]
# train[cols] <- lapply(train[cols],as.factor)


# Transform boolean values - train (also target)
cols <- grep("\\_bin|target",names(train),value=T)
train[, (cols) := lapply(.SD, as.logical), .SDcols=cols]

# Transform categorical values - test
cols <- grep("\\_cat",names(train),value=T)
test[, (cols) := lapply(.SD, as.factor), .SDcols=cols]

# Transform boolean values - test
cols <- grep("\\_bin",names(train),value=T)
test[, (cols) := lapply(.SD, as.logical), .SDcols=cols]

rm(cols)

# Remove id
train <- subset(train,select=c(-id))

# Make backup of train
trainBackup <- train
```

## Exploratory analysis

```{r}
names(train)
str(train)
dim(train)
```

How many complete rows are there?
```{r}
# Absolute
nrow(train[complete.cases(train),])
# Relative
nrow(train[complete.cases(train),]) / nrow(train)
```

## Fitting all variables
```{r}
lmAll <- glm(target~.,data=train)
summary(lmAll)
```

## Feature selection
### Filter method
#### Correlation matrix
Here is a correlation matrix. The following variables are highly correlated (arbitrary cutoff at 0.6):
* **ps_ind_14** and ps_ind_12_bin
* ps_reg_01 and ps_reg_03
* ps_car_13 and ps_car_12
* ps_ind_17_bin and ps_ind_16_bin
* **ps_ind_14** and ps_ind_11_bin

```{r}
# Remove categorical and target column
cols <- grep("\\_cat|target",names(train),value=T,invert=T)

# Generate correlation matrix with absolute value of correlation
corMatrixWide <- abs(cor(train[,..cols]))


# Convert correlation matrix to long format
corMatrixLong <- data.table(melt(corMatrixWide))

# Remove self-correlation rows
corMatrixLong <- corMatrixLong[Var1 != Var2]

# Order from highly-correlate to not correlatied
corMatrixLong <- corMatrixLong[order(-value),]

# Remove dupes (DANGER - what if different covariates have same correlation coefficient?)
corMatrixLong <- corMatrixLong[-seq(0,nrow(corMatrixLong),2),]

# Remove highly correlated values
abundantFeatures <- as.character(unique(corMatrixLong[value > 0.6,]$Var2))
trainF <- train[, (abundantFeatures) := NULL]

rm(cols)
rm(abundantFeatures)
```

#### Decision trees
```{r}
library(rpart)
trainTree <- rpart(target~.,data=trainF,method='class',control=rpart.control(minsplit=10000,cp=0.01))
```

### Wrapper method

## Sampling
```{r}
# For computing speed purposes, code is written by testing it on a 10k non-missing data record data set.
# Uncomment next line to work with complete rows only
train <- train[complete.cases(train),]
# Uncomment the next line to work with a sample of 10k rows.
train <- train[sample(nrow(train),size = 10000,replace = F),]
```

We can use the bestglm or regsubsets package to find out the best variables for each n where n is the amount of predictors. This is slow as hell because it is a huge data set, even when sampled, as it has 2^57 models.

```{r}
library(regsubsets)
# lmReg <- regsubsets(target~.,data=train,nvmax=57,really.big=T)
```

What about forward and backward selection.
```{r}
# Backward
lmBwd <- regsubsets(target~.,data=train,method="backward")
```

### Embedded methods