---
title: "R Notebook"
output: html_notebook
---


```{r}
rm(list=ls())

# Settings
settingDrop = T # Drop variables with high NA rate?
settingImputation = T # Impute NA variables?

set.seed(03031988)
library(data.table)

if (!exists('trainDF')) { trainDF <- data.table(read.csv('train.csv', na.strings="-1")) }
#if (!exists('testDF')) { testDF <- data.table(read.csv('test.csv', na.strings="-1")) }

```

## Data transformation
Info from the Kaggle page:
* The **_bin** suffix means that it is a dummy variable. 
* The **_cat** suffix means that it is a categorical value.
* **-1** means that the value is missing. This has already been taken care of when loading the data.

```{r}
# Transform categorical values - train
cols <- grep("\\_cat",names(trainDF),value=T)
trainDF[, (cols) := lapply(.SD, as.factor), .SDcols=cols]

# Transform boolean values - train (also target)
cols <- grep("\\_bin|target",names(trainDF),value=T)
trainDF[, (cols) := lapply(.SD, as.logical), .SDcols=cols]

# Transform categorical values - test
#cols <- grep("\\_cat",names(trainDF),value=T)
#testDF[, (cols) := lapply(.SD, as.factor), .SDcols=cols]

# Transform boolean values - test
#cols <- grep("\\_bin",names(trainDF),value=T)
#testDF[, (cols) := lapply(.SD, as.logical), .SDcols=cols]

rm(cols)

# Change target classification to factor for caret package
trainDF$target <- as.factor(trainDF$target)

# Remove id
if ("id" %in% names(trainDF)) {
    trainDF <- subset(trainDF,select=c(-id))
}

# Make backup of train
save(trainDF,file="trainDF.rda")
```

## Exploratory analysis

The fact that the claim rate is very low can make the prediction very hard. The _specificity_ will probably be very high, but the _sensitivity_ won't be.

```{r}
# Dimensions of the data frame
dim(trainDF)

# Names of the variables
names(trainDF)

# Summary and overview of the data
str(trainDF)

# What is the 'claim rate'?
nrow(trainDF[target==TRUE]) / nrow(train)

```

How many complete rows are there?

There is also quite a lot of rows which contain NA's. Interesting is the fact that the 'claim rate' is higher for rows where there is no NA values. Also, there are two variables with 69% and 45% of NAs.

```{r}
# Absolute
nrow(trainDF[complete.cases(trainDF),])

# Relative
nrow(trainDF[complete.cases(trainDF),]) / nrow(trainDF)

# Is the amount of NAs spread evenly in relation to the 'claim rate'?
nrow(trainDF[complete.cases(trainDF) & target == TRUE,]) / nrow(trainDF[complete.cases(trainDF),])
nrow(trainDF[!complete.cases(trainDF) & target == TRUE,]) / nrow(trainDF[!complete.cases(trainDF),])

# Which variable has the most NA's?
naCount <- apply(train,2,function(x) sum(is.na(x))) / nrow(trainDF)
naCount <- sort(naCount,decreasing=T)
barplot(head(naCount))

```


## Imputation
In this phase we omit the columns with a very high amount of NAs.
```{r}
library(RANN)
library(caret)

# Drop variables with a high amount of NAs
if (settingDrop == T) {
    trainDF <- trainDF[,c("ps_car_03_cat","ps_car_05_cat") := NULL]
    testDF <- testDF[,c("ps_car_03_cat","ps_car_05_cat") := NULL]
}

```

In this phase we impute the missing data using a KNN method. We also save the meta data - the information that variable Y was missing for row X - in a new variable. We do this for each variable.
```{r}
if (settingImputation == T) {
    if (file.exists("trainImputed.rda")) {
        load(file="trainImputed.rda")
    } else {
        
        # This is a loop that makes sure that the meta data 'field was an NA' is stored in a 
        # separate new dummy for each of the existing variables.
        for (i in 2:as.integer(ncol(trainDF))) {
            isNA <- data.frame(is.na(trainDF[,i,with=F])) # I'm unsure why I need to use 'with' here, I though this was only needed in old data.table versions
            names(isNA) <- paste0("is_na_",names(trainDF)[i])
            if (i == 2) { trainNA <- isNA }
            else { trainNA <- cbind(trainNA,isNA) }
        }
        rm(isNA)
        
        # In a next phase we impute these with a KNN method. Data is also normalized.
        knnImputeModel <- caret::preProcess(trainDF,method="knnImpute")
        trainDF <- predict(knnImputeModel,trainDF)
        save(trainDF,file = "trainImputed.rda")
    }
}
```

Add NA metadata to the imputed data set.
```{r}
    # Combine metadata with training data
    trainDF <- cbind(trainDF,trainNA)
```
## Fitting all variables
For education purposes, we simple modeled a logit model on all the existing variables.

```{r}
# lmAll <- glm(target~.,data=train)
# summary(lmAll)
```

## Feature selection
### Filter method
#### Correlation matrix
Here is a correlation matrix. The following variables are highly correlated (arbitrary cutoff at 0.6):
* **ps_ind_14** and ps_ind_12_bin
* ps_reg_01 and ps_reg_03
* ps_car_13 and ps_car_12
* ps_ind_17_bin and ps_ind_16_bin
* **ps_ind_14** and ps_ind_11_bin

```{r}
# Remove categorical and target column
cols <- grep("\\_cat|target",names(trainDF),value=T,invert=T)

# Generate correlation matrix with absolute value of correlation
corMatrixWide <- abs(cor(trainDF[,..cols]))

# Convert correlation matrix to long format
corMatrixLong <- data.table(melt(corMatrixWide))

# Remove self-correlation rows
corMatrixLong <- corMatrixLong[Var1 != Var2]

# Order from highly-correlate to not correlatied
corMatrixLong <- corMatrixLong[order(-value),]

# Remove dupes (DANGER - what if different covariates have same correlation coefficient?)
corMatrixLong <- corMatrixLong[-seq(0,nrow(corMatrixLong),2),]

# Remove highly correlated values
abundantFeatures <- as.character(unique(corMatrixLong[value > 0.6,]$Var2))
trainF <- trainDF[, (abundantFeatures) := NULL]

# Clean up
rm(cols)
rm(abundantFeatures)
```

#### Decision trees
To find the most important features, we can use decision trees. However, the algorithm does not appear to find any significant features as it is only producing a root. Which is very problematic because decision trees are generally a good way to classify imbalanced classes.

```{r}
library(rpart)
# trainTree <- rpart(target~.,data=trainF,method='class',control=rpart.control(minsplit=1, minbucket=1, cp=0.001))
# plot(trainTree)
```

#### Individual logit regression
In the following section, I run a bivariate logit regression for every variable to determine the importance of each variable individually. This also turns out not to be very informative.

```{r}
glmVars <- data.table(variable = character(),AIC=integer(),deviance=integer(),p=integer())
for (i in 2:ncol(trainF)) {
    v <- names(trainF)[i]
    model <- glm(formula(paste("target~",v)),family="binomial",data=trainF)
    modelSum <- summary(model)
    newrow <- data.table(variable=v,AIC=modelSum$aic,deviance=modelSum$deviance,p=modelSum$coefficients[8])
    glmVars <- rbind(glmVars,newrow)
}
glmVars <- glmVars[order(deviance),]

# Clean up
rm(model)
rm(modelSum)
rm(newrow)
rm(v)
```

### Wrapper method

We can use the bestglm or regsubsets package to find out the best models. This is slow as hell because it is a huge data set, even when sampled, as it has 2^57 models.

```{r}
library(regsubsets)
# lmReg <- regsubsets(target~.,data=train,nvmax=57,really.big=T)
```

What about forward and backward feature selection. Also very slow. Time to go to other solution.

```{r}
# Backward
# lmBwd <- regsubsets(target~.,data=train,method="backward")
```

## Random Forest with Undersampling

The following settings are used:
* na.action = na.omit (no imputation)
* 3 fold cross-valdiation with one repeat
* Undersampling (!)

Sensitivity and specificity around 62% (which is bad).

```{r}
table(trainDF$target)

# ROSE package does not seem to work at all.
# mlr package can't handle logical columns.
# Not a big fan of caret package because it dumbs things down too much. But here goes.
library(caret)
rfModel <- caret::train(target ~ ., data=trainDF, method = "rf", 
                        preProcess = c("scale","center"), na.action = na.omit,
                        trControl = trainControl(method = "repeatedcv",number = 3,
                                                 repeats = 1, verboseIter=T,
                                                 sampling="down")) # Undersampling

rfTrainPredict <- predict(rfModel,trainDF)
confusionMatrix(rfTrainPredict,trainDF[complete.cases(trainDF),]$target)
varImp(rfModel,scale=T)
```